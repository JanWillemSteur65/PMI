{
    "cells": [
        {
            "metadata": {},
            "cell_type": "code",
            "source": "'''\nLicensed Materials - Property of IBM\nIBM Maximo APM - Predictive Maintenance Insights On-Premises\nIBM Maximo APM - Predictive Maintenance Insights SaaS \n\u00a9 Copyright IBM Corp. 2019 All Rights Reserved.\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n'''",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 1,
                    "data": {
                        "text/plain": "'\\nLicensed Materials - Property of IBM\\nIBM Maximo APM - Predictive Maintenance Insights On-Premises\\nIBM Maximo APM - Predictive Maintenance Insights SaaS \\n\u00a9 Copyright IBM Corp. 2019 All Rights Reserved.\\nUS Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\\n'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Maximo APM PMI - Failure Probability Curve Model Template"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "1. [Introduction](#introduction)\n2. [Install Maximo APM PMI SDK](#install-maximo-apm-pmi-sdk)\n3. [Setup the Model Training Pipeline](#setup-model-training-pipline)\n4. [Train the Model Instance](#train-model-instance)\n5. [Register the Trained Model Instance](#register-trained-model-instance)\n6. [Model Template Internals](#model-template-internals)"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id='introduction'></a>\n## Introduction\nStatistically, to evaluate mean life of assets the sample mean or the average age method is acceptable if a big population has end-of-life information. But asset such as generators, transformers, reactors, cables, etc. have a relatively long life up to and even beyond 40 years and generally there are very limited end-of-life failure data. This algorithm is designed to address this use case (to estimate mean life with limited end-of-lift failure data). In fact the proposed algorithm works best when less than 20% of the assets has end-of-life failure data.\n\nIn this notebook, we will be predicting failure probability curve for a kind of assets. In the challenges of asset health assessment, the asset failure probability and its expected remaining life are the key aspects to analysis the asset health status.\n\nThe Failure Probability Curve model uses statistics distribution to assess the failure probability vs. year, and this model has two methods:\n+ **Normal Distribution**: a small percentage of assets fail during the early life cycle, a few last beyond the average expected life span, but the majority fails within their mean life.\n+ **Weibull Distribution**:\n    \\begin{cases} f(x;\\lambda,k) =  \\frac{k}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{k-1}e^{-(x/\\lambda)^{k}} & x\\geq0 ,\\\\ 0 & x<0, \\end{cases}\n    where k > 0 is the shape parameter and \u03bb > 0 is the scale parameter.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"install-maximo-apm-pmi-sdk\"></a>\n## Install Maximo APM PMI SDK"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "To install the SDK, you need your Maximo APM PMI instance ID, API base URL, and your API key. The Maximo APM PMI instance ID and API base URL can be found in the user welcome letter. For API key, request to your Maximo admin to create an user account first to generate one for you. Create one environment variable for each here."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "%%capture\n%env APM_ID=4ac3917e\n%env APM_API_BASEURL=https://prod.pmi.apm.maximo.ibm.com\n%env APM_API_KEY=dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Then, install PMI SDK with `pip`. Note that we have to upgrade `pip` first."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "!pip install -U pip~=18.1\n!pip install pyspark\n!pip install -U https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: pip~=18.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (18.1)\nRequirement already satisfied: pyspark in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark) (0.10.7)\nCollecting https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz\n\u001b[?25l  Downloading https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz (792kB)\n\u001b[K    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 798kB 60.0MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: dill==0.3.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.3.0)\nRequirement already satisfied, skipping upgrade: ibm_db~=3.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (3.0.1)\nRequirement already satisfied, skipping upgrade: ibm_db_sa~=0.3.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.3.4)\nRequirement already satisfied, skipping upgrade: ibmdbpy~=0.1.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.1.5)\nRequirement already satisfied, skipping upgrade: iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0 from https://github.com/ibm-watson-iot/functions/tarball/production_2_0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (2.0.3)\nRequirement already satisfied, skipping upgrade: joblib==0.12.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.12.5)\nRequirement already satisfied, skipping upgrade: lifelines==0.14.6 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.14.6)\nRequirement already satisfied, skipping upgrade: networkx==2.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (2.1)\nRequirement already satisfied, skipping upgrade: numpy~=1.17.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (1.17.5)\nRequirement already satisfied, skipping upgrade: outlier_utils==0.0.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.0.3)\nRequirement already satisfied, skipping upgrade: pandas~=0.24.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.24.1)\nRequirement already satisfied, skipping upgrade: pip~=18.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (18.1)\nRequirement already satisfied, skipping upgrade: requests~=2.18.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (2.18.4)\nRequirement already satisfied, skipping upgrade: requests_futures~=1.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (1.0.0)\nRequirement already satisfied, skipping upgrade: scikit-learn~=0.20.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.20.3)\nRequirement already satisfied, skipping upgrade: scipy==1.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (1.1.0)\nRequirement already satisfied, skipping upgrade: statsmodels==0.9.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.9.0)\nRequirement already satisfied, skipping upgrade: tsfresh==0.10.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (0.10.1)\nRequirement already satisfied, skipping upgrade: watson-machine-learning-client~=1.0.371 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (1.0.376)\nRequirement already satisfied, skipping upgrade: matplotlib in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pmlib==1.0.0) (3.0.2)\nRequirement already satisfied, skipping upgrade: sqlalchemy>=0.7.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm_db_sa~=0.3.3->pmlib==1.0.0) (1.2.18)\nRequirement already satisfied, skipping upgrade: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibmdbpy~=0.1.4->pmlib==1.0.0) (0.17.1)\nRequirement already satisfied, skipping upgrade: pypyodbc in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibmdbpy~=0.1.4->pmlib==1.0.0) (1.3.4)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibmdbpy~=0.1.4->pmlib==1.0.0) (1.12.0)\nRequirement already satisfied, skipping upgrade: lazy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibmdbpy~=0.1.4->pmlib==1.0.0) (1.4)\nRequirement already satisfied, skipping upgrade: lxml==4.3.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (4.3.4)\nRequirement already satisfied, skipping upgrade: psycopg2-binary>=2.8.4 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (2.8.4)\nRequirement already satisfied, skipping upgrade: urllib3==1.22 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (1.22)\nRequirement already satisfied, skipping upgrade: nose>=1.3.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (1.3.7)\nRequirement already satisfied, skipping upgrade: scikit-image>=0.16.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.16.2)\nRequirement already satisfied, skipping upgrade: tabulate==0.8.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.8.5)\nRequirement already satisfied, skipping upgrade: pyod>=0.7.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.7.8)\nRequirement already satisfied, skipping upgrade: ibm-cos-sdk==2.1.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (2.1.3)\nRequirement already satisfied, skipping upgrade: decorator>=4.1.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from networkx==2.1->pmlib==1.0.0) (4.3.2)\nRequirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas~=0.24.0->pmlib==1.0.0) (2018.9)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas~=0.24.0->pmlib==1.0.0) (2.7.5)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests~=2.18.4->pmlib==1.0.0) (3.0.4)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests~=2.18.4->pmlib==1.0.0) (2019.11.28)\nRequirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests~=2.18.4->pmlib==1.0.0) (2.6)\nRequirement already satisfied, skipping upgrade: tqdm>=4.10.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tsfresh==0.10.1->pmlib==1.0.0) (4.31.1)\nRequirement already satisfied, skipping upgrade: patsy>=0.4.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from tsfresh==0.10.1->pmlib==1.0.0) (0.5.1)\nRequirement already satisfied, skipping upgrade: lomond in /opt/conda/envs/Python36/lib/python3.6/site-packages (from watson-machine-learning-client~=1.0.371->pmlib==1.0.0) (0.3.3)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pmlib==1.0.0) (0.10.0)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pmlib==1.0.0) (1.0.1)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from matplotlib->pmlib==1.0.0) (2.3.1)\nRequirement already satisfied, skipping upgrade: setuptools in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pypyodbc->ibmdbpy~=0.1.4->pmlib==1.0.0) (40.8.0)\nRequirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-image>=0.16.2->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (1.0.1)\nRequirement already satisfied, skipping upgrade: imageio>=2.3.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-image>=0.16.2->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (2.4.1)\nRequirement already satisfied, skipping upgrade: pillow>=4.3.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from scikit-image>=0.16.2->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (5.4.1)\nRequirement already satisfied, skipping upgrade: suod in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyod>=0.7.5->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.0.3)\nRequirement already satisfied, skipping upgrade: combo in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyod>=0.7.5->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.1.0)\nRequirement already satisfied, skipping upgrade: numba>=0.35 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyod>=0.7.5->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.42.0)\nRequirement already satisfied, skipping upgrade: ibm-cos-sdk-s3transfer==2.*,>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk==2.1.3->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (2.4.3)\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "Requirement already satisfied, skipping upgrade: ibm-cos-sdk-core==2.*,>=2.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk==2.1.3->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (2.4.3)\nRequirement already satisfied, skipping upgrade: llvmlite>=0.27.0dev0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from numba>=0.35->pyod>=0.7.5->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.27.0)\nRequirement already satisfied, skipping upgrade: docutils>=0.10 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.1.3->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.14)\nRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from ibm-cos-sdk-core==2.*,>=2.0.0->ibm-cos-sdk==2.1.3->iotfunctions@ https://github.com/ibm-watson-iot/functions/tarball/production_2_0->pmlib==1.0.0) (0.9.3)\nBuilding wheels for collected packages: pmlib\n  Running setup.py bdist_wheel for pmlib ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.tmp/pip-ephem-wheel-cache-_cvxcu45/wheels/27/e3/4f/8f4f27f0744ea9362917922e6990c31fc7ed89833aa12f6e3a\nSuccessfully built pmlib\nInstalling collected packages: pmlib\n  Found existing installation: pmlib 1.0.0\n    Uninstalling pmlib-1.0.0:\n      Successfully uninstalled pmlib-1.0.0\nSuccessfully installed pmlib-1.0.0\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"setup-model-training-pipline\"></a>\n## Setup the Model Training Pipeline"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Before you can start working on the model training pipeline, you have to setup an asset group properly in Maximo. See IBM Maximo APM - Predictive MaintenanceInsights SaaS User Guide for details.\n\nRequired model pipeline configuration:\n\n* Asset group ID: The unit of model processing is an asset group. Asset groups are managed on Maximo APM UI. You need to get the ID of the asset group to be analyzed by this model.\n* Asset installation date and decommission date as the label: This model requires asset installation date (Asset attribute **```installdate```** in Maximo) and asset decommission date (Asset attribute **```estendoflife```** in Maximo) to extract the latel for training.\n\nNow you can setup a training pipeine based on this model template, with your own data, to train a model instance."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "from pmlib.degradation_curve import DegradationCurveAssetGroupPipeline\n\ngroup = DegradationCurveAssetGroupPipeline(\n            asset_group_id='1016', \n            model_pipeline={\n                # first feature for training must be asset installation date, and second the asset decommission date\n                \"features_for_training\": [\":installdate\", \":estendoflife\"],\n                \"statistics_distribution_args\": {\n                    \"distribution_type\": \"WEIBULL\", \n                    \"mean_or_scale\": None,\n                    \"stddev_or_shape\": None\n                }\n            })",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Warning: confluent_kafka is not installed. Publish to MessageHub not supported.\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class GraphLasso is deprecated; The 'GraphLasso' was renamed to 'GraphicalLasso' in version 0.20 and will be removed in 0.22.\n  warnings.warn(msg, category=DeprecationWarning)\n",
                    "name": "stderr"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:33.870 pmlib.api.init_environ INFO APM_ID=4ac3917e, APM_API_BASEURL=https://prod.pmi.apm.maximo.ibm.com, APM_API_KEY=********\n2020-03-30T09:46:33.872 pmlib.util.api_request INFO method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/tenant?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json=None, session=None, kwargs={}\n2020-03-30T09:46:35.561 pmlib.util.api_request INFO resp.status_code=200, method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/tenant?instanceId=4ac3917e\n2020-03-30T09:46:35.563 pmlib.api.init_environ DEBUG resp={\n    \"as_apikey\": \"********\",\n    \"as_apitoken\": \"********\",\n    \"as_id\": null,\n    \"as_url\": \"https://api-us.connectedproducts.internetofthings.ibmcloud.com\",\n    \"info\": {\n        \"API_BASEURL\": \"https://api-us.connectedproducts.internetofthings.ibmcloud.com\",\n        \"API_KEY\": \"********\",\n        \"API_TOKEN\": \"********\",\n        \"COS_BUCKET_KPI\": \"analytics-runtime-ctp-pmi-democore-31-7f08111d4f82\",\n        \"COS_BUCKET_LOGGING\": \"analytics-logs-ctp-pmi-democore-31-ad6d973f904e\",\n        \"COS_ENDPOINT\": \"s3-api.us-geo.objectstorage.softlayer.net\",\n        \"COS_HMAC_ACCESS_KEY_ID\": \"********\",\n        \"COS_HMAC_SECRET_ACCESS_KEY\": \"********\",\n        \"COS_REGION\": \"global\",\n        \"DB_CONNECTION_STRING\": \"********\",\n        \"DB_TYPE\": \"db2\",\n        \"MH_BROKERS_SASL\": \"broker-0-xx3x794gyl7j5jjy.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093\",\n        \"MH_DEFAULT_ALERT_TOPIC\": \"analytics-alerts-CTP-PMI-Democore-31\",\n        \"MH_PASSWORD\": \"********\",\n        \"MH_USER\": \"********\",\n        \"PMI_CLOUD_TYPE\": \"PUBLIC\",\n        \"TENANT_ID\": \"CTP-PMI-Democore-31\"\n    },\n    \"keyverified\": \"true\",\n    \"mahi_apikey\": \"********\",\n    \"mahi_auth_url\": \"https://demo-iot.znapz.net/maximo/oslc/permission/allowedappoptions?app=RELENGINEER&lean=1\",\n    \"mahi_id\": \"4ac3917e\",\n    \"mahi_url\": \"https://demo-iot.znapz.net/maximo\",\n    \"tenant_as_id\": \"CTP-PMI-Democore-31\",\n    \"tenant_id\": \"162\",\n    \"tenant_mahi_id\": null,\n    \"tenant_name\": \"CTP_Democore31\",\n    \"wml_cred\": null\n}\n2020-03-30T09:46:35.564 pmlib.api.init_environ INFO setting environment variable MAXIMO_BASEURL=https://demo-iot.znapz.net/maximo\n2020-03-30T09:46:35.574 pmlib.api.init_environ INFO MAXIMO_LINKED=True\n2020-03-30T09:46:35.575 pmlib.api.init_environ INFO setting environment variable MAXIMO_API_CONTEXT=/oslc\n2020-03-30T09:46:35.579 pmlib.api.init_environ INFO setting environment variable SSL_VERIFY_APM=True\n2020-03-30T09:46:35.581 pmlib.api.init_environ INFO setting environment variable SSL_VERIFY_AS=True\n2020-03-30T09:46:35.591 pmlib.api.init_environ INFO setting environment variable SSL_VERIFY_WIOTP=True\n2020-03-30T09:46:35.593 pmlib.api.init_environ INFO setting environment variable SSL_VERIFY_MAXIMO=False\n2020-03-30T09:46:35.596 pmlib.api.init_environ INFO setting environment variable DB_CONNECTION_STRING=********\n2020-03-30T09:46:35.599 pmlib.api.init_environ INFO setting environment variable COS_BUCKET_KPI=analytics-runtime-ctp-pmi-democore-31-7f08111d4f82\n2020-03-30T09:46:35.601 pmlib.api.init_environ INFO setting environment variable COS_BUCKET_LOGGING=analytics-logs-ctp-pmi-democore-31-ad6d973f904e\n2020-03-30T09:46:35.610 pmlib.api.init_environ INFO setting environment variable MH_BROKERS_SASL=broker-0-xx3x794gyl7j5jjy.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093\n2020-03-30T09:46:35.613 pmlib.api.init_environ INFO setting environment variable API_BASEURL=https://api-us.connectedproducts.internetofthings.ibmcloud.com\n2020-03-30T09:46:35.615 pmlib.api.init_environ INFO setting environment variable COS_REGION=global\n2020-03-30T09:46:35.618 pmlib.api.init_environ INFO setting environment variable MH_USER=********\n2020-03-30T09:46:35.621 pmlib.api.init_environ INFO setting environment variable COS_HMAC_ACCESS_KEY_ID=********\n2020-03-30T09:46:35.631 pmlib.api.init_environ INFO setting environment variable COS_HMAC_SECRET_ACCESS_KEY=********\n2020-03-30T09:46:35.634 pmlib.api.init_environ INFO setting environment variable API_TOKEN=********\n2020-03-30T09:46:35.637 pmlib.api.init_environ INFO setting environment variable DB_TYPE=db2\n2020-03-30T09:46:35.639 pmlib.api.init_environ INFO setting environment variable API_KEY=********\n2020-03-30T09:46:35.640 pmlib.api.init_environ INFO setting environment variable PMI_CLOUD_TYPE=PUBLIC\n2020-03-30T09:46:35.643 pmlib.api.init_environ INFO setting environment variable TENANT_ID=CTP-PMI-Democore-31\n2020-03-30T09:46:35.650 pmlib.api.init_environ INFO setting environment variable MH_PASSWORD=********\n2020-03-30T09:46:35.654 pmlib.api.init_environ INFO setting environment variable MH_DEFAULT_ALERT_TOPIC=analytics-alerts-CTP-PMI-Democore-31\n2020-03-30T09:46:35.657 pmlib.api.init_environ INFO setting environment variable COS_ENDPOINT=https://s3-api.us-geo.objectstorage.softlayer.net\n2020-03-30T09:46:35.659 pmlib.util.api_request INFO method=put, url=https://api-us.connectedproducts.internetofthings.ibmcloud.com/api/constants/v1/CTP-PMI-Democore-31, headers={'Content-Type': 'application/json', 'X-api-key': '********', 'X-api-token': '********', 'Cache-Control': 'no-cache'}, timeout=30, ssl_verify=True, json=[{'name': 'apm_info', 'entityType': None, 'enabled': True, 'value': {'APM_ID': '4ac3917e', 'APM_API_BASEURL': 'https://prod.pmi.apm.maximo.ibm.com', 'APM_API_KEY': 'dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f'}, 'metadata': {'type': 'CONSTANT', 'dataType': 'LITERAL', 'description': 'APM tenant information.', 'tags': None, 'required': True, 'values': None}}], session=None, kwargs={}\n2020-03-30T09:46:36.368 pmlib.util.api_request INFO resp.status_code=200, method=put, url=https://api-us.connectedproducts.internetofthings.ibmcloud.com/api/constants/v1/CTP-PMI-Democore-31\n2020-03-30T09:46:36.370 iotfunctions.db.__init__ DEBUG Unable to locate message_hub credentials. Database object created, but it will not be able interact with message hub.\n2020-03-30T09:46:36.371 iotfunctions.db.__init__ DEBUG Found connection string in os variables: DATABASE=BLUDB;HOSTNAME=db2w-ssctxwa.us-south.db2w.cloud.ibm.com;PORT=50001;PROTOCOL=TCPIP;UID=bluadmin;PWD=neCsbd2qUWygbymZv@JiMD8zWD4B@;SECURITY=SSL;\n2020-03-30T09:46:36.374 iotfunctions.db.__init__ DEBUG Found database type in os variables: db2\n2020-03-30T09:46:36.378 iotfunctions.db.__init__ INFO Connection string for SqlAlchemy => db2): db2+ibm_db://bluadmin:neCsbd2qUWygbymZv@JiMD8zWD4B@@db2w-ssctxwa.us-south.db2w.cloud.ibm.com:50001/BLUDB;SECURITY=SSL;\n2020-03-30T09:46:36.381 iotfunctions.db.__init__ DEBUG created a CosClient object\n2020-03-30T09:46:38.795 iotfunctions.db.__init__ DEBUG Db connection established\n2020-03-30T09:46:40.333 iotfunctions.db.http_request DEBUG http request successful. status 200\n2020-03-30T09:46:40.339 analytics_service.pmlib.pipeline._ModelPipelineConfig.__init__ DEBUG features=[], features_for_training=['installdate', 'estendoflife'], predictions=[], features_resampled={}, inputs=[':installdate', ':estendoflife'], renamed_inputs=['installdate', 'estendoflife']\n2020-03-30T09:46:40.342 analytics_service.pmlib.pipeline._ModelPipelineConfig.__init__ DEBUG kwargs={'statistics_distribution_args': {'distribution_type': 'WEIBULL', 'mean_or_scale': None, 'stddev_or_shape': None}}\n2020-03-30T09:46:40.350 iotfunctions.metadata.__init__ DEBUG Initializing new entity type using iotfunctions 2.0.3\n2020-03-30T09:46:40.356 iotfunctions.util.__init__ DEBUG Starting trace\n2020-03-30T09:46:40.357 iotfunctions.util.__init__ DEBUG Trace name: auto_trace_1016_20200330094640\n2020-03-30T09:46:40.360 iotfunctions.util.__init__ DEBUG auto_save None\n2020-03-30T09:46:40.374 iotfunctions.metadata.__init__ WARNING No _db_schema specified in **kwargs. Usingdefault database schema.\n2020-03-30T09:46:40.375 iotfunctions.util.categorize_args DEBUG categorizing arguments\n2020-03-30T09:46:41.901 iotfunctions.metadata.__init__ DEBUG Initialized entity type \n_AssetGroupEntityType:apm_1016\nFunctions:\nGranularities:\nNo schedules metadata\n2020-03-30T09:46:41.905 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.__init__ DEBUG loader_inputs=(':installdate', ':estendoflife'), loader_names=('installdate', 'estendoflife')\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:41.911 analytics_service.pmlib.loader.AssetLoader._validate_data_items DEBUG all_entity_types={'1016', 'IOT', 'IIOT', 'ZPMI', 'ASSET_CACHE', '1011', '1015'}\n2020-03-30T09:46:41.914 analytics_service.pmlib.loader.AssetLoader._set_asset_device_mappings INFO input_asset_device_mappings=None, asset_device_mappings={}, entity_type_meta={}\n2020-03-30T09:46:41.917 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.__init__ DEBUG pipeline_config={'features': [], 'inputs': [':installdate', ':estendoflife'], 'renamed_inputs': ['installdate', 'estendoflife'], 'features_for_training': ['installdate', 'estendoflife'], 'targets': ['installdate', 'estendoflife'], 'predictions': [], 'features_resampled': {}, 'statistics_distribution_args': {'distribution_type': 'WEIBULL', 'mean_or_scale': None, 'stddev_or_shape': None}}\n2020-03-30T09:46:41.921 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.__init__ DEBUG post_processing=[], incremental_summary=True\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The example above configured a pipeline for this model, using asset attribute **```installdate```** and **```estendoflife```** to extract the labels for training. This model only generates the failure probability curve and does not do scoring, hence no predictions defined."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"train-model-instance\"></a>\n## Train the Model Instance\n\nNow you can train the model instance."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = group.execute()",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:41.940 analytics_service.pmlib.cache_loader.AssetCacheRefresher.execute INFO start_ts=None, end_ts=None, entities=None\n2020-03-30T09:46:47.503 pmlib.util.api_request INFO method=get, url=https://api-us.connectedproducts.internetofthings.ibmcloud.com/api/meta/v1/CTP-PMI-Democore-31/entityType/ASSET_CACHE, headers={'Content-Type': 'application/json', 'X-api-key': '********', 'X-api-token': '********', 'Cache-Control': 'no-cache'}, timeout=30, ssl_verify=True, json=None, session=None, kwargs={}\n2020-03-30T09:46:48.233 pmlib.util.api_request INFO resp.status_code=200, method=get, url=https://api-us.connectedproducts.internetofthings.ibmcloud.com/api/meta/v1/CTP-PMI-Democore-31/entityType/ASSET_CACHE\n2020-03-30T09:46:48.237 iotfunctions.metadata.__init__ DEBUG Initializing new entity type using iotfunctions 2.0.3\n2020-03-30T09:46:48.238 iotfunctions.util.__init__ DEBUG Starting trace\n2020-03-30T09:46:48.250 iotfunctions.util.__init__ DEBUG Trace name: auto_trace_ASSET_CACHE_20200330094648\n2020-03-30T09:46:48.254 iotfunctions.util.__init__ DEBUG auto_save None\n2020-03-30T09:46:48.258 iotfunctions.metadata.__init__ WARNING No _db_schema specified in **kwargs. Usingdefault database schema.\n2020-03-30T09:46:48.260 iotfunctions.util.categorize_args DEBUG categorizing arguments\n2020-03-30T09:46:48.263 iotfunctions.metadata.__init__ DEBUG Initialized entity type \nEntityType:apm_asset_cache\nFunctions:\nGranularities:\nNo schedules metadata\n2020-03-30T09:46:48.271 pmlib.util.api_request INFO method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/assetgroup/1016/devicemapping?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json=None, session=None, kwargs={}\n2020-03-30T09:46:49.674 pmlib.util.api_request INFO resp.status_code=200, method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/assetgroup/1016/devicemapping?instanceId=4ac3917e\n2020-03-30T09:46:49.676 pmlib.api._refresh_asset_cache DEBUG raw_asset_device_mappings=[{'devices': ['IIOT:ZIOT1001'], 'assetNum': 'ZIOT1001', 'siteId': 'WCM'}, {'devices': ['IIOT:ZIOT1002'], 'assetNum': 'ZIOT1002', 'siteId': 'WCM'}, {'devices': ['IIOT:ZIOT1003'], 'assetNum': 'ZIOT1003', 'siteId': 'WCM'}, {'devices': ['IIOT:ZIOT1004'], 'assetNum': 'ZIOT1004', 'siteId': 'WCM'}, {'devices': ['IIOT:ZIOT1005'], 'assetNum': 'ZIOT1005', 'siteId': 'WCM'}]\n2020-03-30T09:46:49.700 pmlib.api._refresh_asset_cache DEBUG df_asset_group_members=shape=(5, 4), index={0: 'int64'}, columns={'assetgroup': 'O', 'site': 'O', 'asset': 'O', 'assetid': 'O'}, head(5)=\n  assetgroup site     asset            assetid\n0       1016  WCM  ZIOT1001  ZIOT1001-____-WCM\n1       1016  WCM  ZIOT1002  ZIOT1002-____-WCM\n2       1016  WCM  ZIOT1003  ZIOT1003-____-WCM\n3       1016  WCM  ZIOT1004  ZIOT1004-____-WCM\n4       1016  WCM  ZIOT1005  ZIOT1005-____-WCM\n2020-03-30T09:46:49.715 pmlib.api._refresh_asset_cache DEBUG df_asset_device_mappings=shape=(5, 5), index={0: 'int64'}, columns={'site': 'O', 'asset': 'O', 'devicetype': 'O', 'deviceid': 'O', 'attribute': 'O'}, head(5)=\n  site     asset devicetype  deviceid attribute\n0  WCM  ZIOT1001       IIOT  ZIOT1001          \n1  WCM  ZIOT1002       IIOT  ZIOT1002          \n2  WCM  ZIOT1003       IIOT  ZIOT1003          \n3  WCM  ZIOT1004       IIOT  ZIOT1004          \n4  WCM  ZIOT1005       IIOT  ZIOT1005          \n2020-03-30T09:46:52.501 iotfunctions.pipeline.execute DEBUG Executing pipeline with 2 stages.\n2020-03-30T09:46:52.503 iotfunctions.pipeline.execute DEBUG No dataframe supplied for pipeline execution. Getting entity source data\n2020-03-30T09:46:52.504 analytics_service.pmlib.loader.AssetLoader.execute INFO start_ts=None, end_ts=None, entities=None, df_input=None\n2020-03-30T09:46:52.532 analytics_service.pmlib.loader.AssetLoader._load_asset_device_mappings DEBUG sql=SELECT apm_asset_groups.site, apm_asset_groups.asset, apm_asset_device_attributes.devicetype, apm_asset_device_attributes.deviceid \nFROM apm_asset_groups LEFT OUTER JOIN apm_asset_device_attributes ON apm_asset_device_attributes.site = apm_asset_groups.site AND apm_asset_device_attributes.asset = apm_asset_groups.asset \nWHERE apm_asset_groups.assetgroup = ?\n2020-03-30T09:46:53.076 analytics_service.pmlib.loader.AssetLoader._load_asset_device_mappings DEBUG df_ahi_mappings=shape=(5, 5), index={0: 'int64'}, columns={'site': 'O', 'asset': 'O', 'devicetype': 'O', 'deviceid': 'O', 'assetfullid': 'O'}, head(5)=\n  site     asset devicetype  deviceid        assetfullid\n0  WCM  ZIOT1001       IIOT  ZIOT1001  ZIOT1001-____-WCM\n1  WCM  ZIOT1002       IIOT  ZIOT1002  ZIOT1002-____-WCM\n2  WCM  ZIOT1003       IIOT  ZIOT1003  ZIOT1003-____-WCM\n3  WCM  ZIOT1004       IIOT  ZIOT1004  ZIOT1004-____-WCM\n4  WCM  ZIOT1005       IIOT  ZIOT1005  ZIOT1005-____-WCM\n2020-03-30T09:46:53.079 analytics_service.pmlib.loader.AssetLoader._load_asset_device_mappings DEBUG sql=SELECT apm_asset_groups.site, apm_asset_groups.asset, apm_asset_devices.devicetype, apm_asset_devices.deviceid \nFROM apm_asset_groups LEFT OUTER JOIN apm_asset_devices ON apm_asset_devices.site = apm_asset_groups.site AND apm_asset_devices.asset = apm_asset_groups.asset \nWHERE apm_asset_groups.assetgroup = ?\n2020-03-30T09:46:53.581 analytics_service.pmlib.loader.AssetLoader._load_asset_device_mappings DEBUG df_pmi_mappings=shape=(5, 5), index={0: 'int64'}, columns={'site': 'O', 'asset': 'O', 'devicetype': 'O', 'deviceid': 'O', 'assetfullid': 'O'}, head(5)=\n  site     asset devicetype deviceid        assetfullid\n0  WCM  ZIOT1001       None     None  ZIOT1001-____-WCM\n1  WCM  ZIOT1002       None     None  ZIOT1002-____-WCM\n2  WCM  ZIOT1003       None     None  ZIOT1003-____-WCM\n3  WCM  ZIOT1004       None     None  ZIOT1004-____-WCM\n4  WCM  ZIOT1005       None     None  ZIOT1005-____-WCM\n2020-03-30T09:46:53.586 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}\n2020-03-30T09:46:53.590 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG features_meta={'': {'estendoflife', 'installdate'}}\n2020-03-30T09:46:53.599 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG mappings_meta={'IIOT'}\n2020-03-30T09:46:53.600 analytics_service.pmlib.loader.AssetLoader._validate_mappings WARNING ignore unused entity_type=IIOT in asset_device_mappings, not used in data_items\n2020-03-30T09:46:53.602 analytics_service.pmlib.loader.AssetLoader._set_asset_device_mappings INFO input_asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}, asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}, entity_type_meta={'': ['estendoflife', 'installdate']}\n2020-03-30T09:46:53.637 analytics_service.pmlib.loader.AssetLoader.execute DEBUG df_mappings=shape=(5, 3), index={0: 'int64'}, columns={'asset_id': 'O', 'entity_type': 'O', 'id': 'O'}, head(5)=\n            asset_id entity_type        id\n0  ZIOT1001-____-WCM        IIOT  ZIOT1001\n1  ZIOT1002-____-WCM        IIOT  ZIOT1002\n2  ZIOT1003-____-WCM        IIOT  ZIOT1003\n3  ZIOT1004-____-WCM        IIOT  ZIOT1004\n4  ZIOT1005-____-WCM        IIOT  ZIOT1005\n2020-03-30T09:46:53.639 analytics_service.pmlib.loader.AssetLoader.execute DEBUG entity_type=, data_items=['estendoflife', 'installdate'], is_asset_data=True\n2020-03-30T09:46:53.642 pmlib.util.api_request INFO method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/assetgroup/1016/devicemapping?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json=None, session=None, kwargs={}\n2020-03-30T09:46:54.645 pmlib.util.api_request INFO resp.status_code=200, method=get, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/assetgroup/1016/devicemapping?instanceId=4ac3917e\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:54.651 pmlib.util.api_request INFO method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/pmiboard/assetmeta?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json={'assets': [{'assetNum': 'ZIOT1001', 'siteId': 'WCM'}, {'assetNum': 'ZIOT1002', 'siteId': 'WCM'}, {'assetNum': 'ZIOT1003', 'siteId': 'WCM'}, {'assetNum': 'ZIOT1004', 'siteId': 'WCM'}, {'assetNum': 'ZIOT1005', 'siteId': 'WCM'}], 'attributes': ['estendoflife', 'installdate']}, session=None, kwargs={}\n2020-03-30T09:46:56.517 pmlib.util.api_request INFO resp.status_code=200, method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/pmiboard/assetmeta?instanceId=4ac3917e\n2020-03-30T09:46:56.532 analytics_service.pmlib.loader.AssetLoader.execute DEBUG df_loaded_asset_dimension=shape=(5, 3), index={0: 'int64'}, columns={'asset_id': 'O', 'estendoflife': '<M8[ns]', 'installdate': '<M8[ns]'}, head(5)=\n            asset_id estendoflife installdate\n0  ZIOT1001-____-WCM   2025-02-07  2018-02-07\n1  ZIOT1002-____-WCM   2024-02-07  2018-02-07\n2  ZIOT1003-____-WCM   2023-02-07  2017-02-07\n3  ZIOT1004-____-WCM   2023-02-07  2017-02-07\n4  ZIOT1005-____-WCM   2022-02-07  2015-02-07\n2020-03-30T09:46:56.533 analytics_service.pmlib.loader.AssetLoader.execute DEBUG before merge, df=None\n2020-03-30T09:46:56.538 analytics_service.pmlib.loader.AssetLoader.execute DEBUG before set_index, df=shape=(5, 3), index={0: 'int64'}, columns={'asset_id': 'O', 'estendoflife': '<M8[ns]', 'installdate': '<M8[ns]'}, head(5)=\n            asset_id estendoflife installdate\n0  ZIOT1001-____-WCM   2025-02-07  2018-02-07\n1  ZIOT1002-____-WCM   2024-02-07  2018-02-07\n2  ZIOT1003-____-WCM   2023-02-07  2017-02-07\n3  ZIOT1004-____-WCM   2023-02-07  2017-02-07\n4  ZIOT1005-____-WCM   2022-02-07  2015-02-07\n2020-03-30T09:46:56.572 analytics_service.pmlib.loader.AssetLoader.execute DEBUG df_merged_sorted_n_column_dropped=shape=(5, 2), index={'asset_id': 'O', 'event_timestamp': datetime64[ns, UTC]}, columns={'estendoflife': '<M8[ns]', 'installdate': '<M8[ns]'}, head(5)=\n                                                   estendoflife installdate\nasset_id          event_timestamp                                          \nZIOT1001-____-WCM 2020-03-30 09:42:56.539482+00:00   2025-02-07  2018-02-07\nZIOT1002-____-WCM 2020-03-30 09:43:56.539482+00:00   2024-02-07  2018-02-07\nZIOT1003-____-WCM 2020-03-30 09:44:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1004-____-WCM 2020-03-30 09:45:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1005-____-WCM 2020-03-30 09:46:56.539482+00:00   2022-02-07  2015-02-07\n2020-03-30T09:46:56.573 analytics_service.pmlib.loader.AssetLoader.execute DEBUG fillna=None, fillna_exclude=['estendoflife', 'installdate'], dropna=None, dropna_exclude=['estendoflife', 'installdate']\n2020-03-30T09:46:56.581 analytics_service.pmlib.loader.AssetLoader.execute DEBUG df_final=shape=(5, 2), index={'id': 'O', 'event_timestamp': datetime64[ns, UTC]}, columns={'estendoflife': '<M8[ns]', 'installdate': '<M8[ns]'}, head(5)=\n                                                   estendoflife installdate\nid                event_timestamp                                          \nZIOT1001-____-WCM 2020-03-30 09:42:56.539482+00:00   2025-02-07  2018-02-07\nZIOT1002-____-WCM 2020-03-30 09:43:56.539482+00:00   2024-02-07  2018-02-07\nZIOT1003-____-WCM 2020-03-30 09:44:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1004-____-WCM 2020-03-30 09:45:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1005-____-WCM 2020-03-30 09:46:56.539482+00:00   2022-02-07  2015-02-07\n2020-03-30T09:46:56.595 iotfunctions.pipeline._execute_stage DEBUG Function AssetLoader has no validate_df method. Skipping validation of the dataframe\n2020-03-30T09:46:56.595 iotfunctions.pipeline.execute DEBUG columns excluded when dropping null rows ['deviceid', '_timestamp', 'logicalinterface_id', 'devicetype', 'format', 'updated_utc', 'evt_timestamp']\n2020-03-30T09:46:56.596 iotfunctions.pipeline.execute DEBUG columns considered when dropping null rows ['estendoflife', 'installdate']\n2020-03-30T09:46:56.599 iotfunctions.pipeline.execute DEBUG estendoflife count not null: 5\n2020-03-30T09:46:56.611 iotfunctions.pipeline.execute DEBUG installdate count not null: 5\n2020-03-30T09:46:56.618 iotfunctions.util.log_df_info DEBUG post drop all null rows df count: 5  ; index: id,event_timestamp  ; columns: estendoflife,installdate\n2020-03-30T09:46:56.632 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.execute DEBUG df_input=shape=(5, 2), index={'id': 'O', 'event_timestamp': datetime64[ns, UTC]}, columns={'estendoflife': '<M8[ns]', 'installdate': '<M8[ns]'}, head(5)=\n                                                   estendoflife installdate\nid                event_timestamp                                          \nZIOT1001-____-WCM 2020-03-30 09:42:56.539482+00:00   2025-02-07  2018-02-07\nZIOT1002-____-WCM 2020-03-30 09:43:56.539482+00:00   2024-02-07  2018-02-07\nZIOT1003-____-WCM 2020-03-30 09:44:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1004-____-WCM 2020-03-30 09:45:56.539482+00:00   2023-02-07  2017-02-07\nZIOT1005-____-WCM 2020-03-30 09:46:56.539482+00:00   2022-02-07  2015-02-07\n2020-03-30T09:46:56.634 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.execute DEBUG self.model_timestamp=None\n2020-03-30T09:46:56.642 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.execute DEBUG df_train=shape=(5, 2), index={0: 'int64'}, columns={'installdate': '<M8[ns]', 'estendoflife': '<M8[ns]'}, head(5)=\n  installdate estendoflife\n0  2018-02-07   2025-02-07\n1  2018-02-07   2024-02-07\n2  2017-02-07   2023-02-07\n3  2017-02-07   2023-02-07\n4  2015-02-07   2022-02-07\n2020-03-30T09:46:56.650 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.execute INFO training model None\n2020-03-30T09:46:56.653 analytics_service.pmlib.degradation_curve.DegradationCurve.fit DEBUG initial mean_or_scale=None\n2020-03-30T09:46:56.654 analytics_service.pmlib.degradation_curve.DegradationCurve.fit DEBUG initial stddev_or_shape=None\n2020-03-30T09:46:56.674 analytics_service.pmlib.degradation_curve.DegradationCurve.fit DEBUG df_curve_training_data=shape=(5, 2), index={0: 'int64'}, columns={'installdate': 'int64', 'estendoflife': 'int64'}, df=\n   installdate  estendoflife\n0         2018          2025\n1         2018          2024\n2         2017          2023\n3         2017          2023\n4         2015          2022\n2020-03-30T09:46:56.675 analytics_service.pmlib.degradation_curve.DegradationCurve.fit DEBUG calculate the scale value and shape value for normal distribution...\n2020-03-30T09:46:56.677 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG generate_normal_distribution snapshotyear=None\n2020-03-30T09:46:56.693 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG df_curve_training_data_pre_processing=\n   installdate  estendoflife  retired_flag  date_service\n0         2018          2025             1             7\n1         2018          2024             1             6\n2         2017          2023             1             6\n3         2017          2023             1             6\n4         2015          2022             1             7\n2020-03-30T09:46:56.712 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG df_exposed_table=\n   age  retired_number  exposed_number\n0    7               2               2\n1    6               3               5\nNumber of exposed assets = number of retired assets in max_age, meaning all the asset observed failed at max_age\n2020-03-30T09:46:56.718 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG df_cfp_input=\n   age  retired_number  exposed_number\n1    6               3               5\n2020-03-30T09:46:56.752 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG df_cfp=\n   age  retired_number  exposed_number  pre_cumulative_probability  \\\n0  5.0            -1.0            -1.0                       0.001   \n2  6.0             3.0             5.0                       0.600   \n1  7.0            -1.0            -1.0                       0.000   \n\n   cumulative_probability         z  \n0                   0.001 -3.090232  \n2                   0.601  0.255936  \n1                   0.601  0.255936  \n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:56.757 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG mean_or_scale_NORMAL=6.385270427417803\n2020-03-30T09:46:56.758 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_normal_distribution DEBUG stddev_or_shape_NORMAL=0.4482738804629455\n2020-03-30T09:46:56.773 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG WEIBULL_input_df_cfp=\n   age  retired_number  exposed_number  pre_cumulative_probability  \\\n0  5.0            -1.0            -1.0                       0.001   \n2  6.0             3.0             5.0                       0.600   \n1  7.0            -1.0            -1.0                       0.000   \n\n   cumulative_probability         z   pre_Szx   pre_Szz  \n0                   0.001 -3.090232  2.230779  4.976375  \n2                   0.601  0.255936  0.000000  1.244094  \n1                   0.601  0.255936  1.115390  1.244094  \n2020-03-30T09:46:56.783 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG df_cfp_survival=\n   index  age  retired_number  exposed_number  pre_cumulative_probability  \\\n0      0  5.0            -1.0            -1.0                       0.001   \n1      2  6.0             3.0             5.0                       0.600   \n2      1  7.0            -1.0            -1.0                       0.000   \n\n   cumulative_probability         z   pre_Szx   pre_Szz  survival_probablity  \n0                   0.001 -3.090232  2.230779  4.976375                1.000  \n1                   0.601  0.255936  0.000000  1.244094                0.399  \n2                   0.601  0.255936  1.115390  1.244094                0.399  \n2020-03-30T09:46:58.271 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG initial_beta=19.843000000000018\n2020-03-30T09:46:58.272 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG initial_alpha=6.566970731333391\n2020-03-30T09:46:58.275 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG age_array=[5.0, 6.0, 7.0]\n2020-03-30T09:46:58.276 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG SP_array=[1.0, 0.399, 0.399]\n2020-03-30T09:46:58.279 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG alpha=6.385270427417803\n2020-03-30T09:46:58.283 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG beta=0.4482738804629455\n2020-03-30T09:46:58.291 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG the init performance objective function value is: 0.821194653609014\n2020-03-30T09:46:58.318 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG the optimal alpha is 7.320496071031299\n2020-03-30T09:46:58.337 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG the optimal beta is 2.2536355732207127\n2020-03-30T09:46:58.341 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG mean_or_scale_WEIBULL=7.320496071031299\n2020-03-30T09:46:58.342 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_weibull_distribution DEBUG stddev_or_shape_WEIBULL=2.2536355732207127\n2020-03-30T09:46:58.351 analytics_service.pmlib.degradation_curve.DegradationCurve.fit DEBUG calculate done\n2020-03-30T09:46:58.351 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_final_curve DEBUG generate final degradation curve\n2020-03-30T09:46:58.356 analytics_service.pmlib.degradation_curve.DegradationCurve.generate_final_curve DEBUG final_degradation_curve={0: 0.0, 1: 1.1199467892963222, 2: 5.229280561401184, 3: 12.535419066580033, 4: 22.59605403035132, 5: 34.52558474148053, 6: 47.20318770804353, 7: 59.50662868768457, 8: 70.51982488454898, 9: 79.66403791366682, 10: 86.72982252945589, 11: 91.82078100232512, 12: 95.2448624230252, 13: 97.39586490044117, 14: 98.6582226680358, 15: 99.35030271076329, 16: 99.70468966448065, 17: 99.87412782365155, 18: 99.9497389495832, 19: 99.98121678407023, 20: 99.99343631687923, 21: 99.99785719551899, 22: 99.9993470095942, 23: 99.99981440543124, 24: 99.99995083968187, 25: 99.99998787385415, 26: 99.99999721665222, 27: 99.99999940593203, 28: 99.99999988217837, 29: 99.99999997830106, 30: 99.99999999629158, 31: 99.99999999941225, 32: 99.99999999991367, 33: 99.99999999998825, 34: 99.99999999999852, 35: 99.99999999999983, 36: 99.99999999999997, 37: 100.0, 38: 100.0, 39: 100.0, 40: 100.0, 41: 100.0, 42: 100.0, 43: 100.0, 44: 100.0, 45: 100.0, 46: 100.0, 47: 100.0, 48: 100.0, 49: 100.0, 50: 100.0, 51: 100.0, 52: 100.0, 53: 100.0, 54: 100.0, 55: 100.0, 56: 100.0, 57: 100.0, 58: 100.0, 59: 100.0, 60: 100.0, 61: 100.0, 62: 100.0, 63: 100.0, 64: 100.0, 65: 100.0, 66: 100.0, 67: 100.0, 68: 100.0, 69: 100.0, 70: 100.0, 71: 100.0, 72: 100.0, 73: 100.0, 74: 100.0, 75: 100.0, 76: 100.0, 77: 100.0, 78: 100.0, 79: 100.0, 80: 100.0, 81: 100.0, 82: 100.0, 83: 100.0, 84: 100.0, 85: 100.0, 86: 100.0, 87: 100.0, 88: 100.0, 89: 100.0, 90: 100.0, 91: 100.0, 92: 100.0, 93: 100.0, 94: 100.0, 95: 100.0, 96: 100.0, 97: 100.0, 98: 100.0, 99: 100.0, 100: 100.0}\n2020-03-30T09:46:58.377 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.get_model_extra DEBUG extras=[('apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json', '{\"final_degradation_curve\": {\"0\": 0.0, \"1\": 1.1199467892963222, \"2\": 5.229280561401184, \"3\": 12.535419066580033, \"4\": 22.59605403035132, \"5\": 34.52558474148053, \"6\": 47.20318770804353, \"7\": 59.50662868768457, \"8\": 70.51982488454898, \"9\": 79.66403791366682, \"10\": 86.72982252945589, \"11\": 91.82078100232512, \"12\": 95.2448624230252, \"13\": 97.39586490044117, \"14\": 98.6582226680358, \"15\": 99.35030271076329, \"16\": 99.70468966448065, \"17\": 99.87412782365155, \"18\": 99.9497389495832, \"19\": 99.98121678407023, \"20\": 99.99343631687923, \"21\": 99.99785719551899, \"22\": 99.9993470095942, \"23\": 99.99981440543124, \"24\": 99.99995083968187, \"25\": 99.99998787385415, \"26\": 99.99999721665222, \"27\": 99.99999940593203, \"28\": 99.99999988217837, \"29\": 99.99999997830106, \"30\": 99.99999999629158, \"31\": 99.99999999941225, \"32\": 99.99999999991367, \"33\": 99.99999999998825, \"34\": 99.99999999999852, \"35\": 99.99999999999983, \"36\": 99.99999999999997, \"37\": 100.0, \"38\": 100.0, \"39\": 100.0, \"40\": 100.0, \"41\": 100.0, \"42\": 100.0, \"43\": 100.0, \"44\": 100.0, \"45\": 100.0, \"46\": 100.0, \"47\": 100.0, \"48\": 100.0, \"49\": 100.0, \"50\": 100.0, \"51\": 100.0, \"52\": 100.0, \"53\": 100.0, \"54\": 100.0, \"55\": 100.0, \"56\": 100.0, \"57\": 100.0, \"58\": 100.0, \"59\": 100.0, \"60\": 100.0, \"61\": 100.0, \"62\": 100.0, \"63\": 100.0, \"64\": 100.0, \"65\": 100.0, \"66\": 100.0, \"67\": 100.0, \"68\": 100.0, \"69\": 100.0, \"70\": 100.0, \"71\": 100.0, \"72\": 100.0, \"73\": 100.0, \"74\": 100.0, \"75\": 100.0, \"76\": 100.0, \"77\": 100.0, \"78\": 100.0, \"79\": 100.0, \"80\": 100.0, \"81\": 100.0, \"82\": 100.0, \"83\": 100.0, \"84\": 100.0, \"85\": 100.0, \"86\": 100.0, \"87\": 100.0, \"88\": 100.0, \"89\": 100.0, \"90\": 100.0, \"91\": 100.0, \"92\": 100.0, \"93\": 100.0, \"94\": 100.0, \"95\": 100.0, \"96\": 100.0, \"97\": 100.0, \"98\": 100.0, \"99\": 100.0, \"100\": 100.0}}', False, False)]\n2020-03-30T09:46:58.379 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618\n2020-03-30T09:46:58.382 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json\n2020-03-30T09:46:58.397 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.execute DEBUG df_final=shape=(5, 0), index={'id': 'O', 'event_timestamp': datetime64[ns, UTC]}, columns={}, df=Empty DataFrame\n2020-03-30T09:46:58.399 iotfunctions.pipeline.validate_df WARNING Output dataframe has no columns of type datetime64[ns]. Type has changed or column was dropped.\n2020-03-30T09:46:58.399 iotfunctions.pipeline._execute_stage DEBUG Function DegradationCurveEstimator has no validate_df method. Skipping validation of the dataframe\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:58.412 analytics_service.pmlib.pipeline._ModelPipelineConfig.__init__ DEBUG features=[], features_for_training=['installdate', 'estendoflife'], predictions=[], features_resampled={}, inputs=[], renamed_inputs=[]\n2020-03-30T09:46:58.414 analytics_service.pmlib.pipeline._ModelPipelineConfig.__init__ DEBUG kwargs={'statistics_distribution_args': {'distribution_type': 'WEIBULL', 'mean_or_scale': None, 'stddev_or_shape': None}}\n2020-03-30T09:46:58.417 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.execute DEBUG adjusted after model trained: loader_inputs=[], loader_names=[]\n2020-03-30T09:46:58.419 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}\n2020-03-30T09:46:58.420 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG features_meta={}\n2020-03-30T09:46:58.422 analytics_service.pmlib.loader.AssetLoader._validate_mappings DEBUG mappings_meta={'IIOT'}\n2020-03-30T09:46:58.431 analytics_service.pmlib.loader.AssetLoader._validate_mappings WARNING ignore unused entity_type=IIOT in asset_device_mappings, not used in data_items\n2020-03-30T09:46:58.435 analytics_service.pmlib.loader.AssetLoader._set_asset_device_mappings INFO input_asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}, asset_device_mappings={'ZIOT1001-____-WCM': ['IIOT:ZIOT1001'], 'ZIOT1002-____-WCM': ['IIOT:ZIOT1002'], 'ZIOT1003-____-WCM': ['IIOT:ZIOT1003'], 'ZIOT1004-____-WCM': ['IIOT:ZIOT1004'], 'ZIOT1005-____-WCM': ['IIOT:ZIOT1005']}, entity_type_meta={}\n2020-03-30T09:46:58.436 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.execute DEBUG new_training=True, training_timestamp=1585561618\n2020-03-30T09:46:58.438 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.execute DEBUG self.model_timestamp={'DegradationCurveEstimator': '1585561618'}\n2020-03-30T09:46:58.456 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.execute DEBUG pipeline_final_df=shape=(0, 0), index={'id': 'O', 'event_timestamp': 'O'}, columns={}, df=Empty DataFrame\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Once this method completes successfully, you'll have a trained model instance reday (for next step, see below) and also with the prediction results returned as a dataframe for verification."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"register-trained-model-instance\"></a>\n## Register the Trained Model Instance\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "If the trained model instance looks good, you can register it to Maximo APM PMI:"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "group.register()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:58.473 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG target_pipeilne_class=pmlib.degradation_curve.DegradationCurveAssetGroupPipeline, url=None\n2020-03-30T09:46:58.476 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG catalog_config={'name': 'DegradationCurveAssetGroupPipeline', 'description': 'DegradationCurveAssetGroupPipeline', 'moduleAndTargetName': 'pmlib.degradation_curve.DegradationCurveAssetGroupPipeline', 'url': 'https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz', 'category': 'TRANSFORMER', 'tags': [], 'output': [{'name': 'names', 'description': 'Provide a list of output names to be generated from the pipeline.', 'dataType': 'ARRAY', 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}, 'tags': []}], 'input': [{'name': 'asset_group_id', 'type': 'CONSTANT', 'required': True, 'dataType': 'LITERAL'}, {'name': 'model_pipeline', 'type': 'CONSTANT', 'required': True, 'dataType': 'JSON'}, {'name': 'fillna', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL', 'values': ['backfill', 'bfill', 'ffill', 'pad']}, {'name': 'fillna_exclude', 'type': 'CONSTANT', 'required': False, 'dataType': 'ARRAY', 'dataTypeForArray': ['LITERAL'], 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}}, {'name': 'dropna', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL', 'values': ['any', 'all']}, {'name': 'dropna_exclude', 'type': 'CONSTANT', 'required': False, 'dataType': 'ARRAY', 'dataTypeForArray': ['LITERAL'], 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}}, {'name': 'asset_device_mappings', 'type': 'CONSTANT', 'required': False, 'dataType': 'JSON'}, {'name': 'model_timestamp', 'type': 'CONSTANT', 'required': False, 'dataType': 'JSON'}, {'name': 'local_model', 'type': 'CONSTANT', 'required': False, 'dataType': 'BOOLEAN'}, {'name': 'apm_id', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}, {'name': 'apm_api_baseurl', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}, {'name': 'apm_api_baseurl', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}]}\n2020-03-30T09:46:58.481 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG model_template_body={\"modelTemplateId\": \"DegradationCurveAssetGroupPipeline\", \"modelTemplateName\": \"Failure Probability Curve\", \"modelTemplateDesc\": \"Failure Probability Curve\", \"modelJson\": {\"name\": \"DegradationCurveAssetGroupPipeline\", \"description\": \"DegradationCurveAssetGroupPipeline\", \"moduleAndTargetName\": \"pmlib.degradation_curve.DegradationCurveAssetGroupPipeline\", \"url\": \"https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz\", \"category\": \"TRANSFORMER\", \"tags\": [], \"output\": [{\"name\": \"names\", \"description\": \"Provide a list of output names to be generated from the pipeline.\", \"dataType\": \"ARRAY\", \"jsonSchema\": {\"minItems\": 1, \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}, \"tags\": []}], \"input\": [{\"name\": \"asset_group_id\", \"type\": \"CONSTANT\", \"required\": true, \"dataType\": \"LITERAL\"}, {\"name\": \"model_pipeline\", \"type\": \"CONSTANT\", \"required\": true, \"dataType\": \"JSON\"}, {\"name\": \"fillna\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"LITERAL\", \"values\": [\"backfill\", \"bfill\", \"ffill\", \"pad\"]}, {\"name\": \"fillna_exclude\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"ARRAY\", \"dataTypeForArray\": [\"LITERAL\"], \"jsonSchema\": {\"minItems\": 1, \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, {\"name\": \"dropna\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"LITERAL\", \"values\": [\"any\", \"all\"]}, {\"name\": \"dropna_exclude\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"ARRAY\", \"dataTypeForArray\": [\"LITERAL\"], \"jsonSchema\": {\"minItems\": 1, \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, {\"name\": \"asset_device_mappings\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"JSON\"}, {\"name\": \"model_timestamp\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"JSON\"}, {\"name\": \"local_model\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"BOOLEAN\"}, {\"name\": \"apm_id\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"LITERAL\"}, {\"name\": \"apm_api_baseurl\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"LITERAL\"}, {\"name\": \"apm_api_baseurl\", \"type\": \"CONSTANT\", \"required\": false, \"dataType\": \"LITERAL\"}]}}\n2020-03-30T09:46:58.491 pmlib.util.api_request INFO method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/template?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json={'modelTemplateId': 'DegradationCurveAssetGroupPipeline', 'modelTemplateName': 'Failure Probability Curve', 'modelTemplateDesc': 'Failure Probability Curve', 'modelJson': {'name': 'DegradationCurveAssetGroupPipeline', 'description': 'DegradationCurveAssetGroupPipeline', 'moduleAndTargetName': 'pmlib.degradation_curve.DegradationCurveAssetGroupPipeline', 'url': 'https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/4ac3917e/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/lib/download?filename=pmlib-1.0.0.tar.gz', 'category': 'TRANSFORMER', 'tags': [], 'output': [{'name': 'names', 'description': 'Provide a list of output names to be generated from the pipeline.', 'dataType': 'ARRAY', 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}, 'tags': []}], 'input': [{'name': 'asset_group_id', 'type': 'CONSTANT', 'required': True, 'dataType': 'LITERAL'}, {'name': 'model_pipeline', 'type': 'CONSTANT', 'required': True, 'dataType': 'JSON'}, {'name': 'fillna', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL', 'values': ['backfill', 'bfill', 'ffill', 'pad']}, {'name': 'fillna_exclude', 'type': 'CONSTANT', 'required': False, 'dataType': 'ARRAY', 'dataTypeForArray': ['LITERAL'], 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}}, {'name': 'dropna', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL', 'values': ['any', 'all']}, {'name': 'dropna_exclude', 'type': 'CONSTANT', 'required': False, 'dataType': 'ARRAY', 'dataTypeForArray': ['LITERAL'], 'jsonSchema': {'minItems': 1, '$schema': 'http://json-schema.org/draft-07/schema#', 'type': 'array', 'items': {'type': 'string'}}}, {'name': 'asset_device_mappings', 'type': 'CONSTANT', 'required': False, 'dataType': 'JSON'}, {'name': 'model_timestamp', 'type': 'CONSTANT', 'required': False, 'dataType': 'JSON'}, {'name': 'local_model', 'type': 'CONSTANT', 'required': False, 'dataType': 'BOOLEAN'}, {'name': 'apm_id', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}, {'name': 'apm_api_baseurl', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}, {'name': 'apm_api_baseurl', 'type': 'CONSTANT', 'required': False, 'dataType': 'LITERAL'}]}}, session=None, kwargs={}\n2020-03-30T09:46:59.730 pmlib.util.api_request INFO resp.status_code=200, method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/template?instanceId=4ac3917e\n2020-03-30T09:46:59.732 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG <Response [200]>\n2020-03-30T09:46:59.735 iotfunctions.metadata.register DEBUG found METRIC column deviceid\n2020-03-30T09:46:59.737 iotfunctions.metadata.register DEBUG found METRIC column event_timestamp\n2020-03-30T09:46:59.738 iotfunctions.metadata.register DEBUG found METRIC column devicetype\n2020-03-30T09:46:59.742 iotfunctions.metadata.register DEBUG found METRIC column logicalinterface_id\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:46:59.751 iotfunctions.metadata.register DEBUG found METRIC column eventtype\n2020-03-30T09:46:59.751 iotfunctions.metadata.register DEBUG found METRIC column format\n2020-03-30T09:46:59.756 iotfunctions.metadata.register DEBUG found METRIC column updated_utc\n2020-03-30T09:47:00.440 iotfunctions.db.http_request DEBUG http request successful. status 200\n2020-03-30T09:47:00.441 iotfunctions.metadata.register DEBUG Metadata registered for table apm_1016 \n2020-03-30T09:47:00.444 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.get_model_extra DEBUG extras=[('apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json', '{\"final_degradation_curve\": {\"0\": 0.0, \"1\": 1.1199467892963222, \"2\": 5.229280561401184, \"3\": 12.535419066580033, \"4\": 22.59605403035132, \"5\": 34.52558474148053, \"6\": 47.20318770804353, \"7\": 59.50662868768457, \"8\": 70.51982488454898, \"9\": 79.66403791366682, \"10\": 86.72982252945589, \"11\": 91.82078100232512, \"12\": 95.2448624230252, \"13\": 97.39586490044117, \"14\": 98.6582226680358, \"15\": 99.35030271076329, \"16\": 99.70468966448065, \"17\": 99.87412782365155, \"18\": 99.9497389495832, \"19\": 99.98121678407023, \"20\": 99.99343631687923, \"21\": 99.99785719551899, \"22\": 99.9993470095942, \"23\": 99.99981440543124, \"24\": 99.99995083968187, \"25\": 99.99998787385415, \"26\": 99.99999721665222, \"27\": 99.99999940593203, \"28\": 99.99999988217837, \"29\": 99.99999997830106, \"30\": 99.99999999629158, \"31\": 99.99999999941225, \"32\": 99.99999999991367, \"33\": 99.99999999998825, \"34\": 99.99999999999852, \"35\": 99.99999999999983, \"36\": 99.99999999999997, \"37\": 100.0, \"38\": 100.0, \"39\": 100.0, \"40\": 100.0, \"41\": 100.0, \"42\": 100.0, \"43\": 100.0, \"44\": 100.0, \"45\": 100.0, \"46\": 100.0, \"47\": 100.0, \"48\": 100.0, \"49\": 100.0, \"50\": 100.0, \"51\": 100.0, \"52\": 100.0, \"53\": 100.0, \"54\": 100.0, \"55\": 100.0, \"56\": 100.0, \"57\": 100.0, \"58\": 100.0, \"59\": 100.0, \"60\": 100.0, \"61\": 100.0, \"62\": 100.0, \"63\": 100.0, \"64\": 100.0, \"65\": 100.0, \"66\": 100.0, \"67\": 100.0, \"68\": 100.0, \"69\": 100.0, \"70\": 100.0, \"71\": 100.0, \"72\": 100.0, \"73\": 100.0, \"74\": 100.0, \"75\": 100.0, \"76\": 100.0, \"77\": 100.0, \"78\": 100.0, \"79\": 100.0, \"80\": 100.0, \"81\": 100.0, \"82\": 100.0, \"83\": 100.0, \"84\": 100.0, \"85\": 100.0, \"86\": 100.0, \"87\": 100.0, \"88\": 100.0, \"89\": 100.0, \"90\": 100.0, \"91\": 100.0, \"92\": 100.0, \"93\": 100.0, \"94\": 100.0, \"95\": 100.0, \"96\": 100.0, \"97\": 100.0, \"98\": 100.0, \"99\": 100.0, \"100\": 100.0}}', False, False)]\n2020-03-30T09:47:01.274 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618\n2020-03-30T09:47:02.042 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json\n2020-03-30T09:47:02.930 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_input.gz\n2020-03-30T09:47:03.738 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_input_after_train_preprocess.gz\n2020-03-30T09:47:04.513 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_to_train.gz\n2020-03-30T09:47:05.096 analytics_service.pmlib.degradation_curve.DegradationCurveEstimator.save_model DEBUG saved apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_output.gz\n2020-03-30T09:47:05.097 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG uploaded_models={'DegradationCurveEstimator': ['apm/pmi/model/1016/DegradationCurveEstimator/__1585561618', 'apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json']}\n2020-03-30T09:47:05.098 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG kpi_config={'functionName': 'DegradationCurveAssetGroupPipeline', 'enabled': False, 'input': {'asset_group_id': '1016', 'model_pipeline': {'features_for_training': [':installdate', ':estendoflife'], 'statistics_distribution_args': {'distribution_type': 'WEIBULL', 'mean_or_scale': None, 'stddev_or_shape': None}}, 'incremental_summary': True, 'fillna': None, 'fillna_exclude': None, 'dropna': None, 'dropna_exclude': None, 'local_model': False, 'model_timestamp': {'DegradationCurveEstimator': '1585561618'}}, 'output': {'names': []}, 'backtrack': {'days': 0}}\n2020-03-30T09:47:05.101 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG model_instance_body={\"modelTemplateId\": \"DegradationCurveAssetGroupPipeline\", \"modelCosPath\": {\"DegradationCurveEstimator\": [\"apm/pmi/model/1016/DegradationCurveEstimator/__1585561618\", \"apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json\"]}, \"instanceName\": \"1016_DegradationCurveAssetGroupPipeline_2020-03-30T09:47:05.101607\", \"instanceDesc\": \"1016_DegradationCurveAssetGroupPipeline_2020-03-30T09:47:05.101607\", \"kpiConfig\": {\"functionName\": \"DegradationCurveAssetGroupPipeline\", \"enabled\": false, \"input\": {\"asset_group_id\": \"1016\", \"model_pipeline\": {\"features_for_training\": [\":installdate\", \":estendoflife\"], \"statistics_distribution_args\": {\"distribution_type\": \"WEIBULL\", \"mean_or_scale\": null, \"stddev_or_shape\": null}}, \"incremental_summary\": true, \"fillna\": null, \"fillna_exclude\": null, \"dropna\": null, \"dropna_exclude\": null, \"local_model\": false, \"model_timestamp\": {\"DegradationCurveEstimator\": \"1585561618\"}}, \"output\": {\"names\": []}, \"backtrack\": {\"days\": 0}}, \"granularity\": [{\"name\": \"GroupDaily\", \"description\": \"GroupDaily\", \"frequency\": \"Daily\", \"dataItems\": [], \"entityFirst\": false}, {\"name\": \"Hourly\", \"description\": \"Hourly\", \"frequency\": \"Hourly\", \"dataItems\": [], \"entityFirst\": true}, {\"name\": \"GroupHourly\", \"description\": \"GroupHourly\", \"frequency\": \"Hourly\", \"dataItems\": [], \"entityFirst\": false}, {\"name\": \"Weekly\", \"description\": \"Weekly\", \"frequency\": \"Weekly\", \"dataItems\": [], \"entityFirst\": true}, {\"name\": \"GroupWeekly\", \"description\": \"GroupWeekly\", \"frequency\": \"Weekly\", \"dataItems\": [], \"entityFirst\": false}, {\"name\": \"Monthly\", \"description\": \"Monthly\", \"frequency\": \"Monthly\", \"dataItems\": [], \"entityFirst\": true}, {\"name\": \"GroupMonthly\", \"description\": \"GroupMonthly\", \"frequency\": \"Monthly\", \"dataItems\": [], \"entityFirst\": false}], \"postProcessing\": []}\n2020-03-30T09:47:05.111 pmlib.util.api_request INFO method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/1016/model?instanceId=4ac3917e, headers={'apmapitoken': '********'}, timeout=30, ssl_verify=True, json={'modelTemplateId': 'DegradationCurveAssetGroupPipeline', 'modelCosPath': {'DegradationCurveEstimator': ['apm/pmi/model/1016/DegradationCurveEstimator/__1585561618', 'apm/pmi/model/1016/DegradationCurveEstimator/__1585561618_json']}, 'instanceName': '1016_DegradationCurveAssetGroupPipeline_2020-03-30T09:47:05.101607', 'instanceDesc': '1016_DegradationCurveAssetGroupPipeline_2020-03-30T09:47:05.101607', 'kpiConfig': {'functionName': 'DegradationCurveAssetGroupPipeline', 'enabled': False, 'input': {'asset_group_id': '1016', 'model_pipeline': {'features_for_training': [':installdate', ':estendoflife'], 'statistics_distribution_args': {'distribution_type': 'WEIBULL', 'mean_or_scale': None, 'stddev_or_shape': None}}, 'incremental_summary': True, 'fillna': None, 'fillna_exclude': None, 'dropna': None, 'dropna_exclude': None, 'local_model': False, 'model_timestamp': {'DegradationCurveEstimator': '1585561618'}}, 'output': {'names': []}, 'backtrack': {'days': 0}}, 'granularity': [{'name': 'GroupDaily', 'description': 'GroupDaily', 'frequency': 'Daily', 'dataItems': [], 'entityFirst': False}, {'name': 'Hourly', 'description': 'Hourly', 'frequency': 'Hourly', 'dataItems': [], 'entityFirst': True}, {'name': 'GroupHourly', 'description': 'GroupHourly', 'frequency': 'Hourly', 'dataItems': [], 'entityFirst': False}, {'name': 'Weekly', 'description': 'Weekly', 'frequency': 'Weekly', 'dataItems': [], 'entityFirst': True}, {'name': 'GroupWeekly', 'description': 'GroupWeekly', 'frequency': 'Weekly', 'dataItems': [], 'entityFirst': False}, {'name': 'Monthly', 'description': 'Monthly', 'frequency': 'Monthly', 'dataItems': [], 'entityFirst': True}, {'name': 'GroupMonthly', 'description': 'GroupMonthly', 'frequency': 'Monthly', 'dataItems': [], 'entityFirst': False}], 'postProcessing': []}, session=None, kwargs={}\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "2020-03-30T09:47:09.399 pmlib.util.api_request INFO resp.status_code=200, method=post, url=https://prod.pmi.apm.maximo.ibm.com/ibm/pmi/service/rest/ds/dp7opk78635sbf809f07o4t53lum3c9eoaovpk9f/1016/model?instanceId=4ac3917e\n2020-03-30T09:47:09.401 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG <Response [200]>\n2020-03-30T09:47:09.405 analytics_service.pmlib.degradation_curve.DegradationCurveAssetGroupPipeline.register DEBUG model_instance_response={'modelInstanceId': '1063996D-01F4-49DD-9A8C-564AE30FA41F', 'message': 'APM-PMI-I-0005: Created', 'status': 0}\n",
                    "name": "stdout"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 6,
                    "data": {
                        "text/plain": "'1063996D-01F4-49DD-9A8C-564AE30FA41F'"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Once registration succeeds, you can see this newly trained model instance available for the asset group on IBM Maximo APM - AHI."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"model-template-internals\"></a>\n## Model Template Internals\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Use Case Description\n\nThis model deals with computing failure probabilities vs. year of a kind of assets. Using this model one can answer the questions of the pattern: **What is the failure probability when the asset is N years old ?**\n\n##### Input Data (from Maximo, PMI SDK will do this part)\n\n+ Assets meta data: installation date & decommission date\n\n##### Output\n\nThe output will be a list of asset's year vs. failure probability, which is saved in Cloud Object Storage.\n\n##### Customization Points\n\n+ The **`distribution_type`** in cell **`Setup the Model Training Pipeline`** is the distribution type, should be NORMAL or WEIBULL)\n+ The **`mean_or_scale\"`** and **`stddev_or_shape`** are the parameters for NORMAL or WEIBULL distribution. If not specifed, it will use assets' meta data to calculate the Failure Probability Curve. Otherwise it will generate the curve with specified parameters.\n+ Failure Probability Curve algorithm (**`DegradationCurveEstimator`** Class in below model template code)\n+ Model pipeline stages control (**`DegradationCurveAssetGroupPipeline`** Class in below model template code)\n\n##### Model Workflow and Description (from **`degradation_curve_model.py`** in PMI SDK)\n\n+ Step 1: Collect data of in-service years and retired years of the specific assets\n+ Step 2: Calculate:\n                  Retired (died) one: Age = \u201cRetired year\u201d \u2013 \u201cIn-service year\u201d\n                  Normal one: Age = \u201cCurrent year\u201d \u2013 \u201cIn-service year\u201d\n                  Exposed number: how many reactors service longer than the Age\n+ Step 3: Calculate CFP table:\n                  Failure Probability (FP) = retired number / exposed number\n                  Cumulative FP (k) = Cumulative FP (k-1) + FP (k) \n                  Z values based on Cumulative FP\n                  Create a table following the form like : | Age | Cumulative Failure Probability | z |\n+ Step 4: Estimate the mean life and the standard deviation of the Normal Distribution\n+ Step 5: Generate the NORMAL Degradation Curve\n+ Step 6: based on CFP table to get survival table like: | Age | Survival Probability |\n+ Step 7: Construct Maximum Likelihood function\n+ Step 8: Calculate Initial shape and scale parameters for WEIBULL distribution\n+ Step 9: Use gradient-decent method to get optimal shape and scale parameters"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class DegradationCurveEstimator(BaseEstimator):\n    def __init__(self, features, targets, predictions, statistics_distribution_args=None, **kwargs):\n        super().__init__(features, targets, predictions, **kwargs)\n        self.statistics_distribution_args = statistics_distribution_args\n        self.installdate = kwargs['features_for_training'][0]\n        self.decommissiondate = kwargs['features_for_training'][1]\n\n    def train_model(self, df):\n        # parse statistics_distribution_args\n        distribution_type = self.statistics_distribution_args[\"distribution_type\"]\n        mean_or_scale = self.statistics_distribution_args[\"mean_or_scale\"]\n        stddev_or_shape = self.statistics_distribution_args[\"stddev_or_shape\"]\n\n        # distribution should not be none\n        if distribution_type is None:\n            raise('distribution_type should be NORMAL or WEIBULL')\n        \n        degradation_curve_pipline = DegradationCurve(distribution_type, mean_or_scale, stddev_or_shape, self.installdate, self.decommissiondate)\n        # using sample data to test degradation curve calculation\n        #df_curve_training_data = degradation_curve_pipline.sample_data()\n        # use real data from DB2\n        df_curve_training_data = df\n        self.logger.debug('df_curve_training_data=\\n%s' % df_curve_training_data.head())\n\n        final_degradation_curve = degradation_curve_pipline.fit(df_curve_training_data, distribution_type, mean_or_scale, stddev_or_shape)\n        degradation_curve_model = dict()\n        #degradation_curve_model[\"target\"] =  \"degradation_curve\"\n        degradation_curve_model[\"final_degradation_curve\"] =  final_degradation_curve\n\n        return degradation_curve_model\n    \n    def get_model_extra(self, new_model, model_path):\n        extras = []\n\n        model_json_path = model_path + '_json'\n        extras.append((model_json_path, json.dumps(new_model), False, False)) # no pickle dump, not binary\n\n        self.logger.debug('extras=%s' % str(extras))\n\n        return extras\n\n\nclass DegradationCurveAssetGroupPipeline(AssetGroupPipeline):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.model_template_name = 'Failure Probability Curve'\n\n        self.fillna = None\n        self.dropna = None\n\n    def prepare_execute(self, pipeline, model_config):\n        pipeline.add_stage(DegradationCurveEstimator(**model_config))\n\n    @staticmethod\n    def generate_sample_data(**kwargs):\n        return generate_degradation_curve_data(**kwargs)",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'BaseEstimator' is not defined",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-7-8678148c6025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDegradationCurveEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatistics_distribution_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics_distribution_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistics_distribution_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstalldate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features_for_training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
                    ]
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class DegradationCurve:\n    '''\n    The degradation curve supports NORMAL and WEIBULL distributions\n    distribution_type: the distribution type, NORMAL or WEIBULL\n    mean_or_scale: the mean value of NORMAL or the scale value of WEIBULL\n    stddev_or_shape: the standard deviation value of NORMAL or the shape value of WEIBULL\n    '''\n\n    @classmethod\n    def metadata(cls):\n        return {\n            'name': cls.__name__,\n            'moduleAndTargetName': '%s.%s' % (cls.__module__, cls.__name__),\n            'category': 'TRANSFORMER',\n            'description': 'DegradationCurve',\n            'input': [\n            ],\n            'output': [\n            ],\n            'tags': [\n                'EVENT'\n            ]\n        }\n\n    def __init__(self, data_items, statistics_distribution_args, degradation_curve, installdate, decommissiondate):\n        self.logger = logging.getLogger('analytics_service.%s.%s' % (self.__module__, self.__class__.__name__))\n        self.data_items = data_items\n        self.statistics_distribution_args = statistics_distribution_args\n        self.degradation_curve = degradation_curve\n        self.installdate = installdate\n        self.decommissiondate = decommissiondate\n\n    def execute (self, df):\n        self.logger.debug('input_df=\\n%s' % df.head())\n\n        df_original = df\n\n        # pick only needed columns for scoring \n        df = df[list(set(self.data_items) - set(df.index.names))]\n        self.logger.debug('input_df_pick_need_columns=\\n%s' % df.head())\n\n        sources_not_in_column=df.index.names\n        df = df.reset_index()\n\n        # parse statistics_distribution_args\n        distribution_type = self.statistics_distribution_args[\"distribution_type\"]\n        mean_or_scale = self.statistics_distribution_args[\"mean_or_scale\"]\n        stddev_or_shape = self.statistics_distribution_args[\"stddev_or_shape\"]\n\n        # distribution should not be none\n        if distribution_type is None:\n            raise('distribution_type should be NORMAL or WEIBULL')\n\n        # using sample data to test degradation curve calculation\n        df_curve_training_data = self.sample_data()\n        self.logger.debug('df_curve_training_data=\\n%s' % df_curve_training_data.head())\n\n        # fit the degradation curve\n        df_score = df.astype({self._entity_type._timestamp: 'datetime64[ms]'})[[self._entity_type._df_index_entity_id, self._entity_type._timestamp]].copy()\n        final_degradation_curve = self.fit(df_curve_training_data, distribution_type, mean_or_scale, stddev_or_shape)\n        df_score[self.degradation_curve] = str(final_degradation_curve).strip('[]')\n        self.logger.debug('df_score=\\n%s' % df.head())\n\n        df = df_original.merge(df_score, how='left', left_index=True, right_on=[self._entity_type._df_index_entity_id, self._entity_type._timestamp], sort=False)\n        df = df.set_index(keys=sources_not_in_column)\n        self.logger.debug('df_final=\\n%s' % df.head())\n\n        return df\n\n\n    def fit(self, df_curve_training_data, distribution_type, mean_or_scale, stddev_or_shape):\n        self.logger.debug('initial mean_or_scale=%s' % mean_or_scale)\n        self.logger.debug('initial stddev_or_shape=%s' % stddev_or_shape)\n\n        df_curve_training_data[self.installdate] = pd.to_datetime(df_curve_training_data[self.installdate]).dt.year\n        df_curve_training_data[self.decommissiondate] = np.where(pd.notna(df_curve_training_data[self.decommissiondate]), pd.to_datetime(df_curve_training_data[self.decommissiondate]).dt.year, -1)\n        df_curve_training_data = df_curve_training_data.astype({self.installdate: int, self.decommissiondate: int})\n        self.logger.debug('df_curve_training_data=%s' % log_df_info(df_curve_training_data, head=-1))\n\n        # initialize parameter\n        mean_or_scale_final = 0.0\n        stddev_or_shape_final = 0.0\n\n        # if mean value and stddev value are specified by user, use them to generate the degradation curve\n        if mean_or_scale is not None and stddev_or_shape is not None:\n            self.logger.debug('generate degradation curve with user defined parameters')\n            mean_or_scale_final = float(mean_or_scale)\n            stddev_or_shape_final = float(stddev_or_shape)\n        else:\n            # generate NORMAL distribution by input data\n            if distribution_type == 'NORMAL':\n                self.logger.debug('calculate the mean value and stddev value for normal distribution...')\n                mean_or_scale_final, stddev_or_shape_final, df_cfp = self.generate_normal_distribution(df_curve_training_data)\n                self.logger.debug('calculate done')\n\n            # generate WEIBULL distribution by input data\n            if distribution_type == 'WEIBULL':   \n                self.logger.debug('calculate the scale value and shape value for normal distribution...')\n                mean_or_scale_NORMAL, stddev_or_shape_NORMAL, df_cfp = self.generate_normal_distribution(df_curve_training_data)\n                mean_or_scale_final, stddev_or_shape_final = self.generate_weibull_distribution(mean_or_scale_NORMAL, stddev_or_shape_NORMAL, df_cfp)\n                self.logger.debug('calculate done')\n\n        # return the final curve\n        return self.generate_final_curve(distribution_type, mean_or_scale_final, stddev_or_shape_final)\n                \n    def generate_normal_distribution(self, df_curve_training_data):\n        # df_curve_training_data: [assetId, installationDate, removeDate]\n        # step1. pre-processing\n        date_service = df_curve_training_data[self.decommissiondate] - df_curve_training_data[self.installdate]\n        df_curve_training_data['retired_flag'] = np.where(date_service>=0, 1, 0)\n        df_curve_training_data['date_service'] = np.where(date_service>=0, date_service, 2000 - df_curve_training_data[self.installdate])  # should be datetime.datetime.now().year, 2000 as test year for sample data\n        self.logger.debug('df_curve_training_data_pre_processing=\\n%s' % df_curve_training_data)\n\n        # step2. calculate the exposed table\n        df_exposed_table = df_curve_training_data.groupby(['date_service']).agg({'retired_flag':'sum', 'date_service':'count'})\n        df_exposed_table.rename(columns={'date_service': 'pre_exposed_number'}, inplace=True)\n        df_exposed_table.sort_index(inplace=True, ascending=False)\n        \n        df_exposed_table['exposed_number'] = df_exposed_table.pre_exposed_number.cumsum()\n        del df_exposed_table['pre_exposed_number']\n        df_exposed_table = df_exposed_table.reset_index()\n        df_exposed_table.rename(columns={'retired_flag':'retired_number', 'date_service':'age'}, inplace=True)\n        self.logger.debug('df_exposed_table=\\n%s' % df_exposed_table.head(200))\n\n        # step3. calculate cumulative failure probablity (cfp) table\n        # df_exposed_table: [age, retired_number, exposed_number]\n        max_age = df_exposed_table['age'].max()\n        \n        \n        \n        max_age_idx = df_exposed_table[df_exposed_table['age'] == max_age].index.values.astype(int)[0] # 0, first row\n        #print('------------ ' + str(max_age_idx))\n        a1 = (df_exposed_table.loc[max_age_idx, 'retired_number']) \n        a2 = (df_exposed_table.loc[max_age_idx, 'exposed_number'])\n        if (a1 == a2) : \n            print (\"Number of exposed assets = number of retired assets in max_age, meaning all the asset observed failed at max_age\")\n            # If max_age-1 exist in the df, drop the max_age row as outlier - some asset may survive after max_age year, we just need enough time to observe.\n            # If max_age-1 doesn't exist in the df, modify max_age row to max_age-1\n            if ( (max_age-1) == df_exposed_table.loc[max_age_idx+1, 'age'] ):\n                #print (\"max_age-1 is in !!! drop max_age row\")\n                df_exposed_table = df_exposed_table.drop(df_exposed_table.index[max_age_idx]) \n            else:    \n                df_exposed_table.loc[max_age_idx, 'retired_number'] = 0\n                df_exposed_table.loc[max_age_idx, 'age'] = max_age - 1\n        #print('df_exposed_table 2 =\\n%s' % df_exposed_table.head(200))        \n        #print('------------')\n        \n        \n        \n        df_cfp = df_exposed_table[df_exposed_table['retired_number'] !=0]\n        self.logger.debug('df_cfp_input=\\n%s' % df_cfp.head())\n        df_cfp['pre_cumulative_probability'] = df_cfp['retired_number'] / df_cfp['exposed_number']\n        cfp_first_line = [df_cfp['age'].min()-1, -1, -1, 0.001]\n        cfp_last_line = [max_age, -1, -1, 0]\n        df_cfp = pd.DataFrame(np.array([cfp_first_line, cfp_last_line]), columns=['age', 'retired_number', 'exposed_number', 'pre_cumulative_probability']).append(df_cfp, ignore_index=True)\n        df_cfp.sort_values('age', inplace=True)\n        df_cfp['cumulative_probability'] = df_cfp.pre_cumulative_probability.cumsum()\n        df_cfp['z'] = norm.ppf(df_cfp['cumulative_probability'])\n        self.logger.debug('df_cfp=\\n%s' % df_cfp.head(200))\n\n        # step4. estimate optimal mean value and stddev value\n        #df_cfp: [age, retired_number, exposed_number, pre_cumulative_probability, cumulative_probability, z]\n        age_mean = df_cfp['age'].mean()\n        z_mean = df_cfp['z'].mean()\n        df_cfp['pre_Szx'] = (df_cfp['z'] - df_cfp['z'].mean()) * (df_cfp['age'] - df_cfp['age'].mean())\n        df_cfp['pre_Szz'] = (df_cfp['z'] - df_cfp['z'].mean()) * (df_cfp['z'] - df_cfp['z'].mean())\n        Szx = df_cfp['pre_Szx'].sum()\n        Szz = df_cfp['pre_Szz'].sum()\n        stddev_or_shape_NORMAL = Szx / Szz\n        mean_or_scale_NORMAL = age_mean - stddev_or_shape_NORMAL * z_mean\n        self.logger.debug('mean_or_scale_NORMAL=\\n%s' % mean_or_scale_NORMAL)\n        self.logger.debug('stddev_or_shape_NORMAL=\\n%s' % stddev_or_shape_NORMAL)\n        return mean_or_scale_NORMAL, stddev_or_shape_NORMAL, df_cfp\n\n    def generate_weibull_distribution(self, mean_or_scale_NORMAL, stddev_or_shape_NORMAL, df_cfp):\n        # step1. get the survival table based on cfp table\n        self.logger.debug('WEIBULL_input_df_cfp=\\n%s' % df_cfp)\n        df_cfp['survival_probablity'] = np.where(df_cfp['age'] == df_cfp['age'].min(), 1, 1 - df_cfp['cumulative_probability'])\n        df_cfp_survival = df_cfp.reset_index()\n        self.logger.debug('df_cfp_survival=\\n%s' % df_cfp_survival)\n\n        # step2. get initial alpha (scale) and beta (shape)\n        # initial beta\n        initial_beta = 0.0\n        beta_criteia = sys.float_info.max\n        for beta in np.arange(0.1, 100, 0.001):\n            beta_estimation = ((1+2/beta)**(0.5+2/beta) * math.exp(-(1+2/beta)) * (1 + 1/12/(1+2/beta))) \\\n                                      / ((1+1/beta)**(1+2/beta) * math.exp(-(2+2/beta)) * (1+1/12/(1+1/beta))**2 * math.sqrt(2*math.pi)) \n            beta_criteia_now = abs(beta_estimation - (1 + stddev_or_shape_NORMAL ** 2 / mean_or_scale_NORMAL ** 2))\n            if beta_criteia_now < beta_criteia:\n                beta_criteia = beta_criteia_now\n                initial_beta = beta\n        self.logger.debug('initial_beta=\\n%s' % initial_beta)\n        # initial alpha\n        initial_alpha = math.sqrt(stddev_or_shape_NORMAL**2 / ( math.sqrt(2*math.pi)*((1+2/initial_beta)**(0.5+2/initial_beta) \\\n                                    * math.exp(-(1+2/initial_beta)) * (1 + 1/12/(1+2/initial_beta))) - (2*math.pi*(1+1/initial_beta)**(1+2/initial_beta) \\\n                                    * math.exp(-(2+2/initial_beta)) * (1+1/12/(1+1/initial_beta))**2) ) )\n        self.logger.debug('initial_alpha=\\n%s' % initial_alpha)\n\n        # step3. use gradient decent method to search optimal alpha and beta\n        # initialization\n        age_array = df_cfp_survival['age'].tolist()\n        SP_array = df_cfp_survival['survival_probablity'].tolist() # SP for survivial probability\n        rowcount = len(age_array)\n        alpha = initial_alpha # initial value of alpha\n        beta = initial_beta # initial value of beta\n        eps = 0.01 # step length\n        precision = 0.003 # stop criteria, it is not set too small to avoid overfitting\n        performance_objective = 0 # performance object function\n        gradient_alpha = 0\n        gradient_beta = 0\n        err_objective_iter = 0\n        gradient_alpha_iter = 0\n        gradient_beta_iter = 0\n        \n        # initial value for performance objective function\n        for i in range(0,rowcount):       \n            err_objective_iter = err_objective_iter + math.pow(math.log(SP_array[i]) + math.pow((age_array[i] / alpha), beta),2) \n        performance_objective = err_objective_iter\n        err_objective_iter = 0\n        self.logger.debug(\"the init performance objective function vaule is: \" + str(performance_objective))\n\n        # main part of gradient decent\n        iteration = 0\n        while abs(performance_objective) > precision:          \n            for i in range(0,rowcount):\n                err_objective_iter = err_objective_iter + math.pow(math.log(SP_array[i]) + math.pow((age_array[i] / alpha), beta), 2)\n                gradient_alpha_iter = gradient_alpha_iter + 2 * (math.log(SP_array[i]) + math.pow((age_array[i] / alpha), beta)) * beta * math.pow((age_array[i] / alpha), beta-1) * (-age_array[i] / alpha / alpha)\n                gradient_beta_iter = gradient_beta_iter + 2 * (math.log(SP_array[i]) + math.pow((age_array[i] / alpha), beta)) * math.pow((age_array[i] / alpha), beta) * math.log(age_array[i] / alpha)\n            gradient_alpha = gradient_alpha_iter\n            gradient_alpha_iter = 0\n            gradient_beta = gradient_beta_iter\n            gradient_beta_iter = 0\n            performance_objective = err_objective_iter\n            performance_objective_iter = 0\n            alpha = alpha - eps * gradient_alpha # gradient decent\n            beta = beta - eps * gradient_beta # gradient decent          \n            iteration = iteration + 1\n            #print(iteration)\n            if iteration == 1000 :\n                break\n        self.logger.debug(\"the optimal alpha is \" + str(alpha))\n        self.logger.debug(\"the optimal beta is \" + str(beta))\n\n        mean_or_scale_WEIBULL = alpha\n        stddev_or_shape_WEIBULL = beta\n        self.logger.debug('mean_or_scale_WEIBULL=\\n%s' % mean_or_scale_WEIBULL)\n        self.logger.debug('stddev_or_shape_WEIBULL=\\n%s' % stddev_or_shape_WEIBULL)\n        return mean_or_scale_WEIBULL, stddev_or_shape_WEIBULL\n\n    def generate_final_curve(self, distribution_type, mean_or_scale_final, stddev_or_shape_final):\n        self.logger.debug('generate final degradation curve')\n        #final_degradation_curve = []\n        final_degradation_curve = dict()\n        \n        if distribution_type == 'WEIBULL':\n            for age in range(0, 101, 1):\n                failure_probablity_for_age = (1- math.exp(-((age / mean_or_scale_final) ** stddev_or_shape_final))) * 100  #cumulative density function of WEIBULL\n                #final_degradation_curve.append([age, failure_probablity_for_age])\n                final_degradation_curve[age] = failure_probablity_for_age\n                \n        elif distribution_type == 'NORMAL':\n            for age in range(0, 101, 1):\n                failure_probablity_for_age = norm(mean_or_scale_final, stddev_or_shape_final).cdf(age)  #cumulative density function of NORMAL\n                #final_degradation_curve.append([(]age, failure_probablity_for_age])\n                final_degradation_curve[age] = failure_probablity_for_age\n        else:\n            raise('distribution type is invalid')\n        self.logger.debug('final_degradation_curve=\\n%s' % final_degradation_curve)    \n        # here return the list curve, to consider both old and new pipeline mode\n        #return str(final_degradation_curve).strip('[]')\n        return final_degradation_curve",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "## How to override base class\nIf you want to customize some functions in the model template, you can just override the function. For example **`DegradationCurveEstimator(BaseEstimator)`**, It is based on the base class **`BaseEstimator`** you can:\n+ override the existing method in **`BaseEstimator`**, like **`train_model`** method\n+ add new function to configure the algorithm\n\n    def train_model(self, df):\n        # parse statistics_distribution_args\n        distribution_type = self.statistics_distribution_args[\"distribution_type\"]\n        mean_or_scale = self.statistics_distribution_args[\"mean_or_scale\"]\n        stddev_or_shape = self.statistics_distribution_args[\"stddev_or_shape\"]\n\n        # distribution should not be none\n        if distribution_type is None:\n            raise('distribution_type should be NORMAL or WEIBULL')\n        \n        degradation_curve_pipline = DegradationCurve(distribution_type, mean_or_scale, stddev_or_shape, self.installdate, self.decommissiondate)\n        # using sample data to test degradation curve calculation\n        #df_curve_training_data = degradation_curve_pipline.sample_data()\n        # use real data from DB2\n        df_curve_training_data = df\n        self.logger.debug('df_curve_training_data=\\n%s' % df_curve_training_data.head())\n\n        final_degradation_curve = degradation_curve_pipline.fit(df_curve_training_data, distribution_type, mean_or_scale, stddev_or_shape)\n        degradation_curve_model = dict()\n        #degradation_curve_model[\"target\"] =  \"degradation_curve\"\n        degradation_curve_model[\"final_degradation_curve\"] =  final_degradation_curve\n\n        return degradation_curve_model"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Base class `BaseEstimator`"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "class BaseEstimator(BaseEstimatorFunction):\n    '''Base class for estimators, supporting training/prediction/scoring.\n\n    Note that though the AS base class supports multiple targets per estimator, we \n    are stick to single target per estimator for now. So this class assume the \n    given targets and predictions are always of an 1-element array. Also, when \n    prediction is not needed, the passed in preditions should be an array of one \n    element 'None'.\n    '''\n    def __init__(self, features, targets, predictions, features_for_training=None, label_names=None, **kwargs):\n        super().__init__(features, targets, predictions)\n        self.models = dict()\n        self.model_extras = defaultdict(list)\n        self.logger = get_logger(self)\n        self.features_for_training = features_for_training\n        self.label_names = label_names\n        self.local_model = True\n        self.model_timestamp = None\n        self.training_timestamp = None\n\n    def get_model_name(self, target_name, suffix=None):\n        if suffix is None:\n            suffix = self.model_timestamp\n        return self.generate_model_name(target_name=target_name, prefix=None, suffix=suffix)\n    \n    def generate_model_name(self, target_name, prefix=None, suffix=None):\n        name = ['apm', 'pmi', 'model']\n\n        if prefix is not None:\n            if isinstance(prefix, str):\n                prefix = [prefix]\n            if len(prefix) > 0:\n                name += prefix\n        name.extend([self._entity_type.logical_name, self.name, target_name])\n        name = '/'.join(name)\n\n        if suffix is not None:\n            if isinstance(suffix, datetime):\n                name += '_' + str(calendar.timegm(suffix.timetuple()))\n            else:\n                name += '_' + str(suffix)\n\n        return name\n\n    def _get_target_name(self):\n        return '_' if (self.predictions is None or len(self.predictions) == 0) else self.predictions[0]\n\n    def _load_model(self, bucket):\n        model_name = self.get_model_name(target_name=self._get_target_name()) # load with default suffix timestamp\n        return (model_name, self.load_model(model_name, bucket, self.local_model))\n\n    def load_model(self, model_path, bucket, local):\n        if local:\n            # local FS\n            model = None\n            try:\n                with open(model_path, mode='rb') as file:\n                    model = file.read()\n            except FileNotFoundError as e:\n                pass\n            return pickle.loads(model) if model is not None else None\n        else:\n            return self._entity_type.db.cos_load(filename=model_path, bucket=bucket, binary=True)\n\n    def _save_model(self, bucket, new_model, suffix=None, local=True):\n        filename = self.get_model_name(target_name=self._get_target_name(), suffix=suffix) # save with explicity suffix timestamp set\n\n        objects = [(filename, new_model, True, True)] # model itself always pickle dumped and binary\n        extras = self.get_model_extra(new_model, filename)\n        objects.extend(extras)\n        for fname, obj, picket_dump, binary in objects:\n            self.save_model(obj, fname, bucket, picket_dump, binary, local)\n\n        # add model to internal list for prediction usage\n        self.models[filename] = new_model\n        if len(extras) > 0:\n            self.model_extras[filename].extend(extras)\n\n    def save_model(self, new_model, model_path, bucket, pickle_dump, binary, local):\n        if local:\n            mode = 'w'\n            if pickle_dump:\n                new_model = pickle.dumps(new_model)\n            if binary:\n                mode += 'b'\n\n            try:\n                mkdirp(model_path)\n            except:\n                pass\n            with open(model_path, mode=mode) as file:\n                file.write(new_model)\n        else:\n            try:\n                if pickle_dump:\n                    self._entity_type.db.cos_save(persisted_object=new_model, filename=model_path, bucket=bucket, binary=binary)\n                else:\n                    # work-around to be able to not pickle save to cos\n                    ret = self._entity_type.db.cos_client._cos_api_request('PUT', bucket=bucket, key=model_path, payload=new_model, binary=binary)\n                    if ret is None:\n                        self.logger.warn('Not able to PUT %s to COS bucket %s', (model_path, bucket))\n            except requests.exceptions.ReadTimeout as err:\n                self.logger.warn('timeout saving %s to cos: %s' % (model_path, err))\n\n        self.logger.debug('saved %s' % model_path)\n\n    def get_model_extra(self, new_model, model_path):\n        '''Return extra objects to be saved along with the model as a list of (path, object, pickle_dump, binary) tuplies.\n\n        Normal estimator only has one model object to be saved to COS. Some estimtors might want to save \n        other objects, possibly caching/deriving from the model object, for other usage. You can override \n        this method to return a list of such additional objects, in the form of (cos_path, object, pickle_dump, binary) tuple.\n\n        It is recommended to construct your extra object cos_path based on the given model_path, with \n        different suffix appended.\n        '''\n        return []\n\n    def get_models_for_training(self, db, df, bucket=None):\n        model_name, model = self._load_model(bucket=bucket)\n\n        if model is not None:\n            self.models[model_name] = model\n            return []\n        else:\n            return [model]\n\n    def get_models_for_predict(self, db, bucket=None):\n        if len(self.predictions) == 0 or self.predictions[0] is None:\n            return []\n        else:\n            return list(self.models.values())\n\n    def conform_index(self,df,entity_id_col = None, timestamp_col = None):\n        # workaround for avoiding base class adding columns\n        return df\n\n    def add_training_preprocessor(self, stage):\n        self.add_preprocessor(stage)\n\n    def execute_training_preprocessing(self, df):\n        if len(self._preprocessors) == 0:\n            return df\n        else:\n            return super().execute_preprocessing(df)\n\n    def get_df_for_training(self, df):\n        features = [] + self.features\n        if self.features_for_training is not None:\n            features.extend(self.features_for_training)\n\n        df = df[features]\n        df = df.reset_index(drop=True)\n\n        return df\n\n    def execute_train_test_split(self,df):\n        # TODO disable splitting for now\n        return (df, None)\n\n    def get_df_for_prediction(self, df):\n        df_for_prediction = df[self.features]\n        self.logger.debug('df_for_prediction: %s' % log_df_info(df_for_prediction, head=5))\n        # self.logger.debug('df_for_prediction: %s' % str(df_for_prediction.isna().any(axis='columns')))\n        # df_for_prediction = df_for_prediction.dropna()\n        # self.logger.debug('df_for_prediction_dropna: %s' % log_df_info(df_for_prediction, head=5))\n        return df_for_prediction\n\n    def predict(self, model, df):\n        return list(zip(model.predict(df), model.predict_proba(df))) if model is not None else None\n\n    def get_prediction_result_value_index(self):\n        raise RuntimeError('required but not implemented')\n\n    def process_prediction_result(self, df, prediction_result, model):\n        self.logger.debug('prediction_result_length=%d, prediction_result=%s' % (len(prediction_result), str(prediction_result[:10])))\n\n        if prediction_result is None:\n            df[self.predictions[0]] = None\n\n            self.logger.debug('No suitable model found. Created null predictions')\n        else:\n            for idx in self.get_prediction_result_value_index():\n                if not all([isinstance(p, (tuple, list, np.ndarray)) for p in prediction_result]):\n                    break\n\n                prediction_result = [p[idx] for p in prediction_result]\n\n            df[self.predictions[0]] = prediction_result\n\n            self.logger.debug('final_prediction_result_length=%d, final_prediction_result=%s' % (len(prediction_result), str(prediction_result[:10])))\n\n        return df\n\n    def execute(self, df=None):\n        self.logger.debug('df_input: %s' % log_df_info(df, head=5))\n\n        self.logger.debug('self.model_timestamp=%s' % self.model_timestamp)\n\n        db = self._entity_type.db\n        bucket = self.get_bucket_name()\n\n        self.training_timestamp = None\n\n        # transform incoming data using any preprocessors\n        # include whatever preprocessing stages are required by implementing a set_preprocessors method\n        required_models = self.get_models_for_training(db=db, df=df, bucket=bucket)\n        if len(required_models) > 0:   \n            # Training\n\n            # only do preprocessing and splitting once\n            df_train = self.execute_training_preprocessing(df)\n            df_train = self.get_df_for_training(df_train)\n            self.logger.debug('df_train: %s' % log_df_info(df_train, head=5))\n            # df_train, df_test = self.execute_train_test_split(df_train)\n\n            for model in required_models:\n                self.logger.info('training model: %s' % model) \n\n                new_model = self.train_model(df_train)\n\n                self.training_timestamp = str(calendar.timegm(datetime.utcnow().timetuple()))\n\n                # TODO add evaluation of the new model here, before saving it\n                # if df_test is not None:\n                #     new_model.test(df_test)                \n                #     self.evaluate_and_write_model(new_model = new_model,\n                #                                   current_model = model,\n                #                                   db = db,\n                #                                   bucket=bucket)\n\n                self._save_model(bucket=bucket, new_model=new_model, suffix=self.training_timestamp, local=self.local_model)\n\n                # switch to the new one just trained\n                self.model_timestamp = self.training_timestamp\n        elif self.model_timestamp is not None:\n            self.training_timestamp = self.model_timestamp\n\n        # Predictions\n\n        df_for_prediction = None\n        for idx, model in enumerate(self.get_models_for_predict(db=db, bucket=bucket)):        \n            # TODO deal with multiple predictions\n            if df_for_prediction is None:\n                df_for_prediction = self.get_df_for_prediction(df)\n            df = self.process_prediction_result(df, self.predict(model, df_for_prediction), model)\n\n        if df_for_prediction is None:\n            # no prediction needed, return empty df\n            df = df[[]]\n\n        self.logger.debug('df_final: %s' % log_df_info(df, head=5))\n\n        return df",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}